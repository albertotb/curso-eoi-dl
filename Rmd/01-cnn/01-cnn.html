<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Redes convolucionales</title>
    <meta charset="utf-8" />
    <meta name="author" content="Alberto Torres Barránalberto.torres@icmat.es" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Redes convolucionales
## Programa ejecutivo de Inteligencia Artificial
### Alberto Torres Barrán</br><a href="mailto:alberto.torres@icmat.es" class="email">alberto.torres@icmat.es</a>
### Año de realización: 2019-2020

---




class: middle, center, inverse

# Introducción

---


## Repaso de Deep Learning

* Modelo principal: red profunda (**feed-forward**): composición de **proyecciones lineales** y **no-linealidades**

`\begin{align}
h^{(i+1)} &amp;= Wz^{(i)} + b   \\
z^{(i+1)} &amp;= \sigma (h^{(i+1)}) \\
\end{align}`

* Al final: añadir coste apropiado para regresión o clasificación.

* Las redes neuronales se conocen desde mediados del siglo XX, pero su fuerte resurgimiento no fue hasta esta década:

  * Paralelización en tarjetas gráficas (**GPUs**).
  
  * Librerías de **diferenciación automática**.

---

## Diferenciación Automática (AD)

* ¿Cómo calcular el gradiente en una red profunda?

--

* **A mano**: no escala a nuevas arquitecturas, propenso a errores.

* **Diferenciación numérica**: acumulación de errores y elevado coste computacional.

`\begin{equation}
\frac{\partial E_n}{\partial w_{ji}} = \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji} - \epsilon)}{2 \epsilon} + O(\epsilon^2)
\end{equation}`

* **Diferenciación simbólica**: manipulación exacta de expresiones (mediante tablas de derivadas), pero explosión en la cantidad de términos:

* Surge la **diferenciación automática o algorítmica**: aplica diferenciación simbólica pero solo a expresiones simples, y al componerlas, actualiza los resultados numéricos parciales (que serán **exactos**)


---


## Optimizando mediante SGD

* Una vez hemos calculado el gradiente en un punto mediante AD, las opciones actuales más populares son

* **Descenso por el gradiente estocástico** (SGD):

  1. Muestrear mini-batch
  
  2. Propagar hacia delante (fordward) y calcular función de pérdida
  
  3. Propagar hacia atrás, calculando los gradientes
  
  4. Actualizar los parámetros

* Una **epoca** consiste en repetir la iteración anterior hasta muestrear todos los datos

---

## Volviendo a Deep Learning


* Ya tenemos todos los ingredientes:

  * Conjuntos de datos muy grandes.

  * Redes neuronales como **aproximadores universales**.
  
  * Librerías para **diferenciación automática**: tensorflow, keras, pytorch...
  
  * Potentes CPUs o GPUs para **paralelizar a lo largo de cada ejemplo del mini-batch**.
  
* ¿Qué falta en muchas ocasiones?

--

  * Solo teóricamente las redes neuronales son totalmente expresivas.
  
  * Conviene añadir un **sesgo inductivo** (**inductive bias**) para ayudar al aprendizaje:
  
      * Imágenes, señales: invarianza a traslaciones, escala: **redes convolucionales**.
    
      * Texto, secuencias: sensibilidad al orden de los símbolos: **redes recurrentes**.

---

class: middle, center, inverse

# Redes convolucionales

---

## Introducción

* Tipo de red neuronal para datos con topología similar a una rejilla

  1. 1D, series temporales, audio
  
  2. 2D, imágenes, datos espaciales
  
  3. 3D, video, datos espacio-temporales, meteorología
  
* Red convolucional: en al menos una capa se usan convoluciones en lugar de operaciones con matrices

---

## Motivación

Algunas aplicaciones de redes convolucionales

* Clasificación imágenes: por ej. detección cáncer de piel

* Detección de objetos

* Segmentación de objetos

* Transferencia de estilo

* Colorear imágenes

---

## Detección de objetos

![](img/object_detection.png)

.small[Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 779-788).]

---

## Segmentación de objetos

![](img/object_segmentation.png)

.small[He, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. "Mask R-CNN." In Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969. 2017]

---

## Transferencia de estilo

.center[
![:scale 70%](img/style_transfer.png)
]

.small[Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias. Image style transfer using convolutional neural networks. En Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. p. 2414-2423.]

---

## Colorear imágenes

.center[
![:scale 85%](img/image_colorization.png)
]

.small[Zhang, Richard; Isola, Phillip; Efros, Alexei A. Colorful image colorization. En European conference on computer vision. Springer, Cham, 2016. p. 649-666.]

---

## Clasificar imágenes es complicado

* Cambios en iluminación, contraste, punto de vista

* O simplemente traslaciones:

.center[
![](img/translations.png)
]

---

## Aproximaciones tradicionales

* Crear variables de forma manual

* Preprocesar las imágenes: centrar, recortar, ...

![:vspace 5]

![](img/preprocessing.png)

.small[Raschka, Sebastian. STAT 479: Deep Learning, Spring 2019. Introduction to 
Convolutional Neural Networks Part 1]

---

## Competición ImageNet

* Más de 14 millones de imágenes anotadas a mano

* Más de 20,000 categorias

* Desde 2010, competición anual de clasificación automática (ILSVRC)

  * únicamente 1000 categorias

  * en 2011, el mejor error era de aprox. 25%

  * en 2017, 29/38 equipos tenían un error menor del 5%

---

## Historia

1. En 1990, [Lecun et al.](http://yann.lecun.com/exdb/publis/pdf/lecun-90c.pdf) usa una CNN para leer dígitos de códigos postales
  
  * una de las primeras aplicaciones reales de una red neuronal

  * más del 90% de tasa de acierto
  
2. En 2012, [Krizhevsky et al.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) usan una CNN para ganar la competición ILSVRC2012
  
  * tasa de acierto (top 5), 15.3%
  
  * segundo mejor modelo, 26.2%
  
3. A partir de 2012 múltiples arquitecturas más complejas siguen reduciendo el error: 

  * 2014: VGG-16 (7.3%), GoogleNet (6.7%)
  
  * 2015: Microsoft ResNet (3.57%)

---

## Conexiones densas (*fully connected*)

.center[
![](./img/fullly_connected.png)
]


---

## Conexiones *sparse* (*locally connected*)

.center[
![](./img/locally_connected.svg)
]

---

## Convolución en 2D

* `\(I\)` es la matriz de entrada (2D)

* `\(K\)` es el kernel (2D)

`$$S(i, j) = (K * I)(i,\, j) = \sum_m \sum_n I(i+m,\, j+n) K(m,\, n)$$`

* La convolución o filtro se aplica a toda la imágen con los mismos pesos

* Se define con 4 parámetros:

  * *stride* o paso de la convolución
  
  * tamaño del kernel, generalmente cuadrado
  
  * *depth*, número de filtros o convoluciones distintas a aplicar
  
  * *padding*

---

## Ejemplo

.center[
![:scale 60%](./img/convolution.png)

[Goodfellow et al. Deep Learning (2016)](https://www.deeplearningbook.org/)
]


---

## Características RNN

1. conexiones dispersas

  * explotar estructura espacial
  
  * detectar características locales (aristas, etc.)

2. compartición de pesos

  * un kernel que funciona bien en una región puede que funcione también en otra

  * reduce la cantidad de parámetros

---

## Ejemplo características locales

![](img/edges.png)

* Imagen de la derecha: restar a cada píxel su vecino por la izquierda

* Esta operación se puede representar de forma muy eficiente con una convolución

---

## Compartición de pesos

El kernel se desplaza por la imágen generando un *feature map*

.center[
![](img/kernel1.png)
]

.small[Raschka, Sebastian. STAT 479: Deep Learning, Spring 2019. Introduction to 
Convolutional Neural Networks Part 1]

---

class: center, middle

![](img/kernel2.png)

---

class: center, middle

![](img/kernel3.png)

---

class: center, middle

.large[Varios kernels se usan para obtener múltiples *feature maps*]

![:vspace 2]

![](img/kernel4.png)

---

## Arquitectura ejemplo: LeNet5

Cada conjunto de *feature maps* representa una capa oculta

![](img/lenet5.png)

.small[Yann LeCun, Léon Bottou, Yoshua Bengio and Patrick Haffner: Gradient Based Learning Applied to Document Recognition, Proceedings of IEEE, 86(11):2278–2324, 1998]

---

## Convolución con distintos canales

![](img/cnn_color_channels.png)

---

## Número de parámetros

![](img/cnn_num_parameters.png)

---

## Parámetros del kernel

1. Tamaño del kernel (alto x ancho, suelen ser iguales)

2. *Stride* o paso

3. *Padding*

  - cuando no se aplica *padding* se habla de *"valid" convolution*

---

## *Stride* (paso)

.center[
![:scale 90%](./img/Stride1.png)

[Fuente](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)]

---

## Ejemplos

.pull-left[

* Entrada `\(4 \times 4\)`
* Kernel `\(3 \times 3\)`
* Stride 1
* Salida `\(2 \times 2\)`

![](./img/no_padding_no_strides.gif)
]

.pull-right[

* Entrada `\(5 \times 5\)`
* Kernel `\(3 \times 3\)`
* Stride 2
* Salida `\(2 \times 2\)`

![](./img/no_padding_strides.gif)
]

---

## Padding 

* En ocasiones se añade un *padding* de 0 al borde de la imágen:

  1. preservar el tamaño de la entrada (*"same" convolution*)
  
  2. cuando es necesario por la combinación de tamaño de entrada, tamaño de kernel y stride
  
* Ejemplo: entrada `\(5\times 5\)`, kernel `\(3 \times 3\)` y *stride* 2

.center[
![](./img/numerical_padding_strides.gif)

[Fuente](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)]

---

* Generalmente la salida tiene menor tamaño que la entrada

* Aplicando padding podemos hacer que tenga el mismo o incluso mayor

* Entrada `\(5 \times 5\)`, stride 1, kernel `\(3 \times 3\)`
  
.pull-left[
![](img/same_padding_no_strides.gif)

.center[
Salida `\(5 \times 5\)`
]
]

.pull-right[
![](img/full_padding_no_strides.gif)
.center[
Salida `\(7 \times 7\)`
]

]

---

* A veces el padding es necesario para poder aplicar el kernel

* Ejemplo: kernel `\(3 \times 4\)`, stride 2

* La salida tiene el mismo tamaño en ambos!

.pull-left[
![](img/padding_strides.gif)

.center[
Entrada `\(5\times 5\)`
]
]

.pull-right[
![](img/padding_strides_odd.gif)

.center[
Entrada `\(6 \times 6\)`
]
]


---

## Pooling

* Capa de submuestreo no lineal

* Previene sobreajuste, reduciendo número de parámetros

* Ayuda con la invarianza frente a traslaciones

* Útil cuando interesa conocer si una característica está o no, pero no su localización exacta `\(\Rightarrow\)` clasificación imágenes

* Más habitual: **max pooling**

--

![:vspace 1]

.center[
![](./img/Max_pooling.png)
]

---

## Visualizando activaciones

.center[
![:scale 30%](img/cat.png)

![:scale 70%](img/first_layer_example.png)
]

---

## Primeras capas

![](img/first_layer.png)

---

## Últimas capas

![](img/last_layer.png)

---

## CAM (class activation map)

.center[
![:scale 80%](img/CAM.png)
]
.small[Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp; Torralba, A. (2016). Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929).]

---

class: inverse, center, middle

# Arquitecturas

---

## Arquitectura típica

* Parámetros: número de *feature maps*, tamaño del kernel, stride

* Subsampling: max pooling

* Antes de las capas *fully connected*, hay que aplanar (*flatten*) la salida

![:vspace 5]

![](./img/Typical_cnn.png)

---

## Arquitecturas comunes

.center[
![:scale 80%](img/common_arch.png)
]

.small[Canziani, A., Paszke, A., &amp; Culurciello, E. (2016). An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678.]

---

## Ejemplo arquitectura: AlexNet

![](img/alexnet.png)


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_conv_2d(filters = 96, 
                kernel_size = c(11, 11), 
                activation = 'relu',
                input_shape = c(224, 224, 3), 
                strides = c(4, 4), 
                padding = 'valid') %&gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2), 
                       strides = c(2, 2), 
                       padding = 'valid') %&gt;%
  ...
```

---

## Ejemplo arquitectura: VGG-16

.center[
![:scale 130%](img/vgg16.png)
]

.small[Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014)]

---

## ResNets


* Se añaden conexiones entre capas usando la función identidad (*skip connection*), de forma que se pueden "saltar" capas que no son útiles

* Este truco permite implementar arquitecturas muy profundas


.center[
![:scale 75%](img/resnet.png)
]

.small[He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016]

---

## R vs Python


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_conv_2d(filters = 96, 
                kernel_size = c(11, 11), 
                activation = 'relu',
                input_shape = c(224, 224, 3), 
                strides = c(4, 4), 
                padding = 'valid') %&gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2), 
                       strides = c(2, 2), 
                       padding = 'valid') %&gt;%
  ...
```


```python
model = Sequential()

model.add(Conv2D(filters=96, 
                 input_shape=(224,224,3), 
                 kernel_size=(11,11), 
                 strides=(4,4), 
                 padding='valid'))

model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2,2), 
                       strides=(2,2), 
                       padding='valid'))
...
```


---

class: middle, center, inverse

# Recursos adicionales

---

## Enlaces de interés

* https://reddit.com/r/LearnMachineLearning: nivel introductorio/medio

* https://reddit.com/r/machinelearning: discusiones sobre artículos y temas de actualidad

* https://medium.com/topic/machine-learning: artículos hacia audiencia general

* https://github.com/rasbt/stat479-deep-learning-ss19: curso deep learning
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
